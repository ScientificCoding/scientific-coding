{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997acde4-b934-420e-a2e5-53235f9ecdf6",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f32c4-f731-45d9-81e5-d44f37d746a7",
   "metadata": {},
   "source": [
    "This notebook demonstrates a method to encode token positions in a sequence into tensors as a way of injecting positional information into model flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d23730-0c56-42a0-9d49-60ff8a920f1d",
   "metadata": {},
   "source": [
    "![alt text](embedding_flow.png \"Embedding Flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309da30-59e9-41ee-a45d-e64936fdc5e4",
   "metadata": {},
   "source": [
    "To include positional information, we need a way of encoding the position of a token in the same dimension as the token embedding in order to combine the two tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec7b3c-d1ac-42a2-8246-73c0176532b0",
   "metadata": {},
   "source": [
    "For given inputs\n",
    "\n",
    "- <b>pos</b>: position of the token in the sequence\n",
    "- <b>d</b>: dimension of the encoded output\n",
    "- and 0 <= <b>i</b> <= d/2\n",
    "\n",
    "the positional encoding compute as follows:\n",
    "\n",
    "![alt text](formula.png \"Positional Encoding Formula\")\n",
    "\n",
    "Source: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acea77e-bee1-4ce9-91fe-4802914397ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = !pip install torch\n",
    "_ = !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29cc8e-cb26-4bbe-b7f2-3fe729d1f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50768b87-b578-411e-8ef5-20291a5be4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_out: int):\n",
    "        super().__init__()\n",
    "        self.dim_out = dim_out\n",
    "        self.n = 10000\n",
    "\n",
    "    def forward(self, pos):\n",
    "        even = torch.arange(0, 2 * self.dim_out / 2, 2).view(1, -1).repeat(pos.size(0), 1)\n",
    "        odd = torch.arange(1, 2 * self.dim_out / 2 + 1, 2).view(1, -1).repeat(pos.size(0), 1)\n",
    "        pos = pos.view(-1, 1)\n",
    "        even = torch.sin(pos / torch.pow(self.n, (even / self.dim_out)))\n",
    "        odd = torch.cos(pos / torch.pow(self.n, (odd / self.dim_out)))\n",
    "        out = torch.zeros(pos.shape[0], self.dim_out)\n",
    "        out[:, 0::2] = even\n",
    "        out[:, 1::2] = odd\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d53d7-7b39-46d4-bed1-0f5a1e2fb21f",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5bd935-9139-4daf-9576-2fdebc3b7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100\n",
    "sequence_length = 200\n",
    "\n",
    "pe = PositionalEncoding(d)\n",
    "x = torch.tensor(range(0, sequence_length))\n",
    "y = pe(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017bdb2-0c3b-480f-a504-a4ef4c735491",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bb1b1-ac4c-4cc1-9769-937d88d94888",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y[5].numpy(), label=\"5\")\n",
    "plt.plot(y[6].numpy(), label=\"6\")\n",
    "plt.plot(y[50].numpy(), label=\"50\")\n",
    "plt.xlabel('encoding dimension')\n",
    "plt.ylabel('encoding value')\n",
    "plt.legend(title=\"position\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f48487-d4c2-4b5c-a534-476a16c451e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cax = plt.matshow(y.numpy().transpose(), cmap='Purples')\n",
    "plt.gcf().colorbar(cax)\n",
    "plt.xlabel('token position')\n",
    "plt.ylabel('encoding dimension')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
