{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01b496d-b5ed-42e3-aa26-01a8d790d2f7",
   "metadata": {},
   "source": [
    "# Diffusion Models\n",
    "## Train image generators in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c428f-5b72-400b-87fb-b3562074791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = !pip install torch\n",
    "_ = !pip install torchvision\n",
    "_ = !pip install matplotlib\n",
    "_ = !pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e20cad-598a-4ac1-8d6e-b6712b89db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageColor, Image, ImageDraw\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58124470-d36d-4540-a8c7-6fbd38d9516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = 300\n",
    "TRAIN_IMAGE_SIZE = 64\n",
    "DATA_DIR = \"./data\"\n",
    "TRAIN_DATA_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "os.makedirs(TRAIN_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a743c2-3a14-43da-99d7-8e1e8ba28552",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fa4b09-1516-4a27-a02d-8201ab7780ba",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "The below functions create images of circles with varying radii and locations and store them in a designated folder. These images will be used as training data for our image generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774faa7d-c0a4-43ee-8053-ff7b59966441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_circle_image() -> Image:\n",
    "    \"\"\"\n",
    "    Draw a 64 x 64 image of a circle of random radius.\n",
    "    \"\"\"\n",
    "    image_size = (TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE)\n",
    "    image = Image.new(\"RGB\", image_size, \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    radius = random.randint(10, 25)\n",
    "    x_shift = random.randint(-10, 10)\n",
    "    y_shift = random.randint(-10, 10)\n",
    "    center_x = x_shift + image_size[0] // 2 \n",
    "    center_y = y_shift + image_size[1] // 2\n",
    "    color_names = list(ImageColor.colormap.keys())\n",
    "    color_names.remove(\"white\")\n",
    "    color = random.choice(color_names)\n",
    "    draw.ellipse((center_x - radius, center_y - radius, center_x + radius, center_y + radius), fill=color)\n",
    "    return image\n",
    "\n",
    "\n",
    "def create_image_file_name(index: int) -> str:\n",
    "    max_length_name_index = len(str(N_TRAIN))\n",
    "    name = f\"{index}\"\n",
    "    while len(name) < max_length_name_index:\n",
    "        name = \"0\" + name\n",
    "    name += \".png\"\n",
    "    return name\n",
    "    \n",
    "\n",
    "for i in range(N_TRAIN):\n",
    "    name = create_image_file_name(i)\n",
    "    save_path = os.path.join(TRAIN_DATA_DIR, name)\n",
    "    image = draw_random_circle_image()\n",
    "    image.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921a948-5927-4883-8fb3-c0050a97f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_train_images():\n",
    "\n",
    "    n_images = 10\n",
    "    n_cols = 5\n",
    "    n_rows = 2\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        index = random.randint(0, N_TRAIN - 1)\n",
    "        image_file_name = create_image_file_name(index)\n",
    "        image_path = os.path.join(TRAIN_DATA_DIR, image_file_name)\n",
    "        image = Image.open(image_path)\n",
    "        ax.imshow(image)\n",
    "\n",
    "plot_random_train_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6c565-1796-4fc3-8fdf-14d3106a4fe3",
   "metadata": {},
   "source": [
    "## Dataset for Training\n",
    "The images can be organized into a data set which makes it more convenient to work through during training. In addition it offers a way to define a sequence of transformations to be applied to the images.\n",
    "\n",
    "In our case, we need to scale the pixel values to a range of -1 to 1. As of now, their values range from 0-255 for each pixel color (RGB). The Pytorch toTensor() operation scales them automatically to a range of 0-1. Therefore, we apply one more function to shift the values into the desired range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c583a-b576-4eda-acf4-4e177d07896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pix = np.array(image)\n",
    "plt.figure(figsize=(3, 3))\n",
    "_ = plt.hist(pix.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fbffe-64d3-4d58-a89f-828e03ea6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_transform_dataset(data_dir: str = DATA_DIR) -> ImageFolder:\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1)\n",
    "    ])\n",
    "    data_set = ImageFolder(data_dir, transform=data_transforms)\n",
    "    return data_set\n",
    "\n",
    "train_dataset = load_and_transform_dataset()\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00614058-ee5d-46e9-9285-480bad3a0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, label_0 = train_dataset[0]\n",
    "x_0 = x_0.to(device)\n",
    "print(type(x_0))\n",
    "print(x_0.shape)\n",
    "_ = plt.hist(x_0.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe1cfb-ed8c-47c2-8635-e7b5db514fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(xt: torch.Tensor) -> Image:\n",
    "    \"\"\"\n",
    "    Revert the transformations on the tensor and return corresponding Image\n",
    "    \"\"\"\n",
    "\n",
    "    xt = xt.cpu()\n",
    "\n",
    "    if len(xt.shape) == 4 and xt.shape[0] == 1:\n",
    "        xt = xt.squeeze()\n",
    "    \n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    return reverse_transforms(xt)\n",
    "\n",
    "image = tensor_to_image(x_0)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae307f-f0c3-4a9c-8a62-2652182d655c",
   "metadata": {},
   "source": [
    "## Apply Noise to Images\n",
    "In this forward process we apply noise to an image over the course of numerous steps. The added noise increases by step (so-called noise-schedule). In the end of the step process, the image will be complete uniformly distributed noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2cd6b7-b338-4ffd-af56-b9867bdb0fab",
   "metadata": {},
   "source": [
    "![noise equation](./noise_equation.png \"Noise Equation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37d530-8fd5-4488-bef6-767c5ce4cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "betas = torch.linspace(start=0.0001, end=0.01, steps=n_steps, device=device)\n",
    "alphas = 1. - betas\n",
    "alphas_bar = torch.cumprod(alphas, axis=0)\n",
    "sqrt_alphas_bar = torch.sqrt(alphas_bar)\n",
    "sqrt_one_minus_alphas_bar = torch.sqrt(1. - alphas_bar)\n",
    "\n",
    "\n",
    "def apply_noise(xt: torch.Tensor, t: torch.Tensor, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply noise to an image tensor at a giving time step t according to the noise schedule beta\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(xt, device=device)\n",
    "    \n",
    "    sqrt_alphas_bar_t = sqrt_alphas_bar.gather(-1, t).reshape(t.shape[0], 1, 1, 1)\n",
    "    sqrt_one_minus_alphas_bar_t = sqrt_one_minus_alphas_bar.gather(-1, t).reshape(t.shape[0], 1, 1, 1)\n",
    "\n",
    "    noisy_image = sqrt_alphas_bar_t.to(device) * xt.to(device) + sqrt_one_minus_alphas_bar_t.to(device) * noise.to(device)\n",
    "\n",
    "    return noisy_image, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff834d-5a54-4c3e-b35d-9456924949b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0_batch = x_0.unsqueeze(0).repeat(5, 1, 1, 1)\n",
    "t = torch.linspace(start=0, end=n_steps-1, steps=5, device=device).long()\n",
    "x_noise_t, noise_t = apply_noise(xt=x_0_batch, t=t, device=device)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(18, 3))\n",
    "x_all = torch.cat((x_0.unsqueeze(0), x_noise_t), dim=0)\n",
    "for i, ax in enumerate(axes):\n",
    "    image = tensor_to_image(x_all[i])\n",
    "    ax.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5727a7-572e-4af1-9dc9-e29430e39cde",
   "metadata": {},
   "source": [
    "## Representation and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d0bb1-341e-4196-b770-0f8c47ec1186",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "The images are well represented by the tensors we have transformed them into. \n",
    "For the time \"positions\" reprenting the step index in the noise sequence we need an encoding which transforms sequential integers into fixed-dimensional vector representations. \n",
    "For details on that process, please see my video and notebook on that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03466ab-c964-4cd6-8473-f2b6fa3a89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Position Encodings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_out: int):\n",
    "        super().__init__()\n",
    "        self.dim_out = dim_out\n",
    "        self.n = 10000\n",
    "\n",
    "    def forward(self, pos: torch.Tensor):\n",
    "        even = torch.arange(0, 2 * self.dim_out / 2, 2).view(1, -1).repeat(pos.size(0), 1).to(device)\n",
    "        odd = torch.arange(1, 2 * self.dim_out / 2 + 1, 2).view(1, -1).repeat(pos.size(0), 1).to(device)\n",
    "        pos = pos.view(-1, 1)\n",
    "        even = torch.sin(pos / torch.pow(self.n, (even / self.dim_out)))\n",
    "        odd = torch.cos(pos / torch.pow(self.n, (odd / self.dim_out)))\n",
    "        out = torch.zeros(pos.shape[0], self.dim_out).to(device)\n",
    "        out[:, 0::2] = even\n",
    "        out[:, 1::2] = odd\n",
    "        return out     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef9bc5-3d67-4351-b956-9ab467756467",
   "metadata": {},
   "source": [
    "## Unet Model\n",
    "The Unet model is U-shaped in the sense that it \"down-samples\" a tensor via convoluations and afterwards \"up-samples\" via de-convolutions. The ouput of a Unet is therefore of the same dimension as the input. This makes is a suited model for infering a tensor of noise pixels for an input image tensor.\n",
    "We add the time encoding vector in the process in order to include the information at which step in the noise process we are in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a09a56-4c73-4494-bc54-7502a21072f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_encoding_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_linear =  nn.Linear(in_features=time_encoding_dim, out_features=out_channels)\n",
    "        \n",
    "        self.conv1 = None\n",
    "        self.transform = None\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        h = self.batch_norm1(self.relu(self.conv1(x)))\n",
    "        time_encoding = self.relu(self.time_linear(t))\n",
    "        time_encoding = time_encoding.unsqueeze(-1).unsqueeze(-1)\n",
    "        h = h + time_encoding\n",
    "        h = self.batch_norm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class UpBlock(AbstractBlock):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_encoding_dim: int):\n",
    "        super().__init__(in_channels, out_channels, time_encoding_dim)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=2*in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.transform = nn.ConvTranspose2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "\n",
    "class DownBlock(AbstractBlock):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_encoding_dim: int):\n",
    "        super().__init__(in_channels, out_channels, time_encoding_dim)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.transform = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e924aa5-df1b-43c2-be7b-6fdcd9c04f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet architecture\n",
    "\n",
    "    (Time) position encodings (32 dim)\n",
    "    + 5 layers of convolutional downsampling\n",
    "    + 5 layers of convolutional upsampling\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3 \n",
    "        time_pos_dim = 32\n",
    "\n",
    "        self.pos_linear = nn.Sequential(\n",
    "                PositionalEncoding(time_pos_dim),\n",
    "                nn.Linear(time_pos_dim, time_pos_dim),\n",
    "                nn.ReLU()\n",
    "            ).to(device)\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(in_channels=image_channels, out_channels=down_channels[0], kernel_size=3, stride=1, padding=1, device=device)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([DownBlock(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_pos_dim) \\\n",
    "                    for i in range(len(down_channels)-1)]).to(device)\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([UpBlock(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_pos_dim) \\\n",
    "                    for i in range(len(up_channels)-1)]).to(device)\n",
    "        \n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1).to(device)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        t = self.pos_linear(timestep)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)           \n",
    "            x = up(x, t)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c62118-3641-4626-9c48-e7144557c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = x_0.to(device)\n",
    "x_0_un = x_0.unsqueeze(0)\n",
    "pos = torch.tensor([5], device=device).unsqueeze(0)\n",
    "unet = Unet()\n",
    "unet(x_0_un, pos).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e56e5-bbfe-49be-a043-d680fb066348",
   "metadata": {},
   "source": [
    "## Generate an Image\n",
    "Given a model which can estimate the noise added at a given timestep, we can generate an image that is representative of the training set. We start at a noisy image, predict its added noise and subtract it. We repeat this process n_steps times and will end with a generated de-noised image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e683941e-5035-46d3-b89b-6c2baea87ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_one_over_alphas = torch.sqrt(1.0 / alphas)\n",
    "alphas_bar_prev = F.pad(alphas_bar[:-1], (1, 0), value=1.0)\n",
    "posterior_variance = betas * (1. - alphas_bar_prev) / (1. - alphas_bar)\n",
    "\n",
    "@torch.no_grad()\n",
    "def denoised_image_at_timestemp(model: nn.Module, x: torch.Tensor, t: int):\n",
    "    t_tensor = torch.Tensor([t]).to(device)\n",
    "    noise_pred = model(x, t_tensor)\n",
    "    denoised_x = sqrt_one_over_alphas[t] * (x - betas[t] * noise_pred / sqrt_one_minus_alphas_bar[t])\n",
    "    if t == 0:\n",
    "        return denoised_x\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return denoised_x + torch.sqrt(posterior_variance[t]) * noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75787d0-b3b1-472c-8d06-9f6f4f6f5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sequential_denoising(model: nn.Module):\n",
    "    img = torch.randn((1, 3, TRAIN_IMAGE_SIZE, TRAIN_IMAGE_SIZE), device=device)\n",
    "    \n",
    "    n_images = 6\n",
    "    show_image_at = int(n_steps/n_images)\n",
    "\n",
    "    for t in range(n_steps-1, -1, -1):\n",
    "        img = denoised_image_at_timestemp(model, img, t)\n",
    "        img = torch.clamp(img, -1.0, 1.0)\n",
    "\n",
    "        if t % show_image_at == 0:\n",
    "            image = tensor_to_image(img)\n",
    "            plt.figure()\n",
    "            plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c2581-03ee-4627-a31d-446a3715f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet().to(device)\n",
    "sequential_denoising(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee001a0e-90f4-458b-a6fc-3acc8ef7a522",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "We apply a standard deep learning flow to train the model. We use L1 loss and and Adam Optimizer to tune the model weights. For details on the training flow, please see my video and notes on that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7663624-d78c-4f89-9ef9-c5f237a849cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "score_every = 10\n",
    "\n",
    "model = Unet().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        x = batch[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        t = torch.randint(0, n_steps, (x.shape[0],), device=device).long()\n",
    "        x_noise_t, noise_t = apply_noise(xt=x, t=t, device=device)\n",
    "        noise_pred = model(x_noise_t, t)\n",
    "        loss = F.l1_loss(noise_t, noise_pred)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if epoch % score_every == 0 and step == 0:\n",
    "            print(f\"epoch {epoch} --- loss: {loss.item()} \")\n",
    "            \n",
    "\n",
    "sequential_denoising(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e435a-08bd-41d5-bdc1-e2c105c24bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
