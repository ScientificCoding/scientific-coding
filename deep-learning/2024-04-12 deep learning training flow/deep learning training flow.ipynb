{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9756656-66c3-4130-a2d9-c461b708f944",
   "metadata": {},
   "source": [
    "# Deep Learning Training Flow\n",
    "- Create our own sample data\n",
    "- Construct a multilayer neural network classifier\n",
    "- Select optimizer and loss function\n",
    "- Train\n",
    "- Evaluate \n",
    "- Set up hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604d7e8-1d0c-428c-9436-83ae71bf332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = !pip install torch\n",
    "_ = !pip install matplotlib\n",
    "_ = !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae96a8-b9f8-4534-a2a6-9922b72c79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81323b-e97c-4564-8a23-22e0686b48ea",
   "metadata": {},
   "source": [
    "### Mock data generation\n",
    "- X: 2D cartesian data\n",
    "    - x,y coordinates\n",
    "- Labels:\n",
    "    - \"1\" if point inside a circle with radius 0.5\n",
    "    - \"0\" if point is outside circle\n",
    " \n",
    "Data should be stratified for best training results, meaning there should be an approximately even number of data points representing \"0\" and \"1\" labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb70cb4-ebc5-4720-8820-3cda0761968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_data(n_records: int, n_features: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    def _is_inside_circle(_x: np.ndarray) -> int:\n",
    "        if np.sqrt(np.sum(np.square(_x))) < 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    _x = (np.random.random(size=(n_records, n_features)) - 0.5)\n",
    "    _labels = np.apply_along_axis(_is_inside_circle, axis=1, arr=_x)\n",
    "\n",
    "    return _x, _labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80570e74-3f82-4c10-9ed2-65bb52934acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, labels = create_mock_data(10000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaaba96-18ee-4d36-9d6e-2340c3df0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=x[:, 0], y=x[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa9e4c-c14c-428a-9cdc-82501dda43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(_x: np.ndarray, _labels: np.ndarray, n_each: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    unique_labels = np.unique(_labels)\n",
    "    row_indices = np.arange(_x.shape[0])\n",
    "    sample_x = []\n",
    "    sample_labels = []\n",
    "    for unique_label in unique_labels:\n",
    "        _idx = np.random.choice(row_indices[np.where(_labels == unique_label)], size=n_each, replace=False)\n",
    "        sample_x.append(_x[_idx])\n",
    "        sample_labels.append(_labels[_idx])\n",
    "        \n",
    "    return np.concatenate(sample_x), np.concatenate(sample_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0de1c-8776-477a-935d-8a7d5af8db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_mock_data(n_records: int, n_features: int):\n",
    "    pool_x, pool_labels = create_mock_data(n_records=10*n_records, n_features=n_features)\n",
    "    return stratified_sample(pool_x, pool_labels, n_each=n_records//2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421497f-32e7-4690-a522-7c5542632693",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, labels = create_stratified_mock_data(n_records=200, n_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a82233-0a8a-424a-a8c0-21495bbc00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=x[:, 0], y=x[:, 1], c=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5008fb2-b809-492f-9a7f-2cc53f6ffa4f",
   "metadata": {},
   "source": [
    "### Create Pytorch data sets for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234ba05-73eb-41f8-8de3-da5423b8ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x: np.ndarray, labels: np.ndarray):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6323d-c69b-4ec7-a645-431db11e227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "training_data_set = MockDataset(*create_stratified_mock_data(n_records=10000, n_features=n_features))\n",
    "validation_data_set = MockDataset(*create_stratified_mock_data(n_records=500, n_features=n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53c84d-0e56-4881-9fe8-c0695e48e4be",
   "metadata": {},
   "source": [
    "### Classification Model\n",
    "- linear neural network\n",
    "- variable number of hidden layers and number of nodes per hidden layer\n",
    "- relu activation\n",
    "- optional batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c42398-326c-4129-9956-643e8cf789bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features: int, n_hidden_layers: int, nodes_per_layer: int, include_batch_norm: bool, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.include_batch_norm = include_batch_norm\n",
    "        self.input_layer = nn.Linear(n_features, nodes_per_layer).to(device)\n",
    "        self.hidden_layers = [nn.Linear(nodes_per_layer, nodes_per_layer).to(device) for _ in range(n_hidden_layers)]\n",
    "        if include_batch_norm:\n",
    "            self.batch_norms = [nn.BatchNorm1d(nodes_per_layer).to(device) for _ in range(n_hidden_layers)]\n",
    "        self.output_layer = nn.Linear(nodes_per_layer, 2).to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_layer(x)\n",
    "        x = torch.relu(x)\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            x = self.hidden_layers[i](x)\n",
    "            if self.include_batch_norm:\n",
    "                x = self.batch_norms[i](x)\n",
    "            x = torch.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self(x)\n",
    "        probs = torch.softmax(input=y, dim=1)\n",
    "        _, labels = torch.max(probs, dim=1)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f3d30-6386-4e38-9518-951415c313cd",
   "metadata": {},
   "source": [
    "### Compute Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415efae2-f017-499a-a64a-d0fcba27302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_loss(model: nn.Module, loss_function: nn.Module, dataset: Dataset, device: torch.device):\n",
    "    \"\"\"\n",
    "    Compute loss of model for complete dataset in eval mode (no gradients)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data_loader = DataLoader(dataset, batch_size=500)\n",
    "        cumulative_loss = 0.0\n",
    "        for x, labels in data_loader:\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y_hat = model(x)\n",
    "            cumulative_loss += loss_function(y_hat, labels).item()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return cumulative_loss / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670b112-34f2-4b34-8975-76ed2569197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_confusion_matrix(model: nn.Module, dataset: Dataset, device: torch.device):\n",
    "    \"\"\"\n",
    "    Create a confusion matrix on the dataset in eval mode (no gradients)\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data_loader = DataLoader(dataset, batch_size=500)\n",
    "        actual = []\n",
    "        predicted = []\n",
    "        for x, labels in data_loader:\n",
    "            x = x.to(device)\n",
    "            labels_pred = model.predict(x)\n",
    "            actual.append(labels.cpu().numpy())\n",
    "            predicted.append(labels_pred.cpu().numpy())\n",
    "    model.train()\n",
    "\n",
    "    actual = np.concatenate(actual)\n",
    "    predicted = np.concatenate(predicted)\n",
    "\n",
    "    return confusion_matrix(actual, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238daab-29cc-4cc2-8285-9e0f4010b842",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494175a-d497-4c77-b821-d77c219587be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs: int, \n",
    "          learning_rate: float,\n",
    "          batch_size: int,\n",
    "          n_hidden_layers: int, \n",
    "          nodes_per_layer: int, \n",
    "          include_batch_norm: bool, \n",
    "          eval_every: int) -> nn.Module:\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = MLP(n_features, n_hidden_layers, nodes_per_layer, include_batch_norm, device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    data_loader = DataLoader(training_data_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, labels in data_loader:\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = loss_function(y_hat, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % eval_every == 0:\n",
    "            validation_loss = compute_dataset_loss(model, loss_function, validation_data_set, device)\n",
    "            print(f\"Epoch: {epoch} --- validation loss: {validation_loss}\")\n",
    "\n",
    "    print(compute_dataset_confusion_matrix(model, validation_data_set, device))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efbb5b1-ac7d-4f6d-a784-6bccdf39c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train(epochs=10, \n",
    "          learning_rate=0.01,\n",
    "          batch_size=32,\n",
    "          n_hidden_layers=2,\n",
    "          nodes_per_layer=4, \n",
    "          include_batch_norm=False,\n",
    "          eval_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366f42e-421b-4374-896a-2445eb182ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train(epochs=10, \n",
    "          learning_rate=0.01,\n",
    "          batch_size=32,\n",
    "          n_hidden_layers=2,\n",
    "          nodes_per_layer=4, \n",
    "          include_batch_norm=True,\n",
    "          eval_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee093d1-d363-42ea-8b93-0bb82247c771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d982a5-df93-47d2-887a-b88ebb2eb4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd485d-7618-45c9-b405-81b67537b7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
